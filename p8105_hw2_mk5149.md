p8105_hw2_mk5149
================
Mungyu Kwok
2025-09-30

# Preparation

Firstly, we will load any necessary packages.

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(lubridate)
library(readxl)
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'
    ## 
    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

``` r
library(knitr)
```

Next, we will load the datasets (Fivethirtyeight), and save them into
dataframe.

``` r
pols_df = read_csv("fivethirtyeight_datasets/pols-month.csv")
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_df = read_csv("fivethirtyeight_datasets/snp.csv")
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemployment_df = read_csv("fivethirtyeight_datasets/unemployment.csv")
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

# Problem 1

First, clean the data in `pols-month.csv`. We will use the `separate()`
function to break up the variable `mon` into integer variables `year`,
`month`, and `day`. Then, we will replace month number with month name;
We will also create a president variable taking values `gop` and `dem`,
and remove `prez_dem` and `prez_gop`; Finally, we will remove the `day`
variable.

``` r
pols_clean = 
  pols_df |>
  separate(mon, into = c("year","month","day"), sep = '-', convert = TRUE) |> 
  mutate(
    month = month.abb[month], #buildt-in R constant
    president_party = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) |>
  select(-day, -prez_gop, -prez_dem) # Removes only the day, prez_gop, and prez_dem columns
pols_clean
```

    ## # A tibble: 822 × 9
    ##     year month gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president_party
    ##    <int> <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>          
    ##  1  1947 Jan        23      51     253      23      45     198 dem            
    ##  2  1947 Feb        23      51     253      23      45     198 dem            
    ##  3  1947 Mar        23      51     253      23      45     198 dem            
    ##  4  1947 Apr        23      51     253      23      45     198 dem            
    ##  5  1947 May        23      51     253      23      45     198 dem            
    ##  6  1947 Jun        23      51     253      23      45     198 dem            
    ##  7  1947 Jul        23      51     253      23      45     198 dem            
    ##  8  1947 Aug        23      51     253      23      45     198 dem            
    ##  9  1947 Sep        23      51     253      23      45     198 dem            
    ## 10  1947 Oct        23      51     253      23      45     198 dem            
    ## # ℹ 812 more rows

Secondly, we will clean the data in `snp.csv` by using a similar process
to the above. For consistency across datasets, arrange according to year
and month, and organize so that `year` and `month` are the leading
columns.

``` r
snp_clean = 
  snp_df |>
  # 1. To be safe, first ensure the date column is in character format
  mutate(date_str = as.character(date)) |>
  # 2. Split the date string by "/" into month, day, and year columns
  separate(date_str, into = c("month_val", "day_val", "year_val"), sep = "/", remove = FALSE) |>
  # 3. Convert the year from a character to a number
  mutate(year_num = as.numeric(year_val)) |>
  # 4. Manually fix the century: if the 2-digit year is > 25 (a reasonable cutoff), assume it's a 19xx year; otherwise, assume it's a 20xx year
  mutate(full_year = ifelse(year_num > 25, 1900 + year_num, 2000 + year_num)) |>
  # 5. Create a new, correct date using the fixed full year and the original month and day
  mutate(corrected_date = make_date(year = full_year, month = month_val, day = day_val)) |>
  # 6. From this corrected date, extract the final year and month we need
  mutate(
    year = year(corrected_date),
    month = month(corrected_date, label = TRUE, abbr = TRUE) |> as.character()
  ) |>
  # 7. Finally, select only the required columns
  select(year, month, snp_close = close)

snp_clean
```

    ## # A tibble: 787 × 3
    ##     year month snp_close
    ##    <dbl> <chr>     <dbl>
    ##  1  2015 Jul       2080.
    ##  2  2015 Jun       2063.
    ##  3  2015 May       2107.
    ##  4  2015 Apr       2086.
    ##  5  2015 Mar       2068.
    ##  6  2015 Feb       2104.
    ##  7  2015 Jan       1995.
    ##  8  2014 Dec       2059.
    ##  9  2014 Nov       2068.
    ## 10  2014 Oct       2018.
    ## # ℹ 777 more rows

Thirdly, we will tidy the unemployment data so that it can be merged
with the previous datasets. This process will involve switching from
“wide” to “long” format; ensuring that key variables have the same name;
and ensuring that key variables take the same values.

``` r
unemployment_clean = 
  unemployment_df |>
  pivot_longer(
    cols = Jan:Dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |> 
  rename(year = Year) # Add this line to fix the capitalization, which will cause error in the later part (merging the data)
unemployment_clean
```

    ## # A tibble: 816 × 3
    ##     year month unemployment_rate
    ##    <dbl> <chr>             <dbl>
    ##  1  1948 Jan                 3.4
    ##  2  1948 Feb                 3.8
    ##  3  1948 Mar                 4  
    ##  4  1948 Apr                 3.9
    ##  5  1948 May                 3.5
    ##  6  1948 Jun                 3.6
    ##  7  1948 Jul                 3.6
    ##  8  1948 Aug                 3.9
    ##  9  1948 Sep                 3.8
    ## 10  1948 Oct                 3.7
    ## # ℹ 806 more rows

For the final part of problem 1, we will join the datasets by merging
`snp` into `pols`, and merging `unemployment` into the result.

``` r
fivethirtyeight_df = 
  pols_clean |>
  left_join(snp_clean, by = c("year","month")) |>
  left_join(unemployment_clean, by = c("year", "month"))
#left_join: based on shared columns, if no matches -> NA
```

The three raw datasets from FiveThirtyEight provided distinct economic
and political metrics over time. The `pols-month.csv` dataset contained
monthly presidential approval ratings from 1947 to 2017, with indicators
for the president’s political party. The `snp.csv` dataset included the
monthly closing value of the S&P 500 index. Lastly, the
`unemployment.csv` dataset was in a wide format, listing the national
unemployment rate for each month in separate columns, covering years
from 1948 to 2015.

For the specific information for these 3 datasets, please refer to the
summary below:

``` r
summary(pols_clean)
```

    ##       year         month              gov_gop         sen_gop    
    ##  Min.   :1947   Length:822         Min.   :12.00   Min.   :32.0  
    ##  1st Qu.:1964   Class :character   1st Qu.:18.00   1st Qu.:42.0  
    ##  Median :1981   Mode  :character   Median :22.00   Median :46.0  
    ##  Mean   :1981                      Mean   :22.48   Mean   :46.1  
    ##  3rd Qu.:1998                      3rd Qu.:28.00   3rd Qu.:51.0  
    ##  Max.   :2015                      Max.   :34.00   Max.   :56.0  
    ##     rep_gop         gov_dem        sen_dem         rep_dem   
    ##  Min.   :141.0   Min.   :17.0   Min.   :44.00   Min.   :188  
    ##  1st Qu.:176.0   1st Qu.:22.0   1st Qu.:48.00   1st Qu.:211  
    ##  Median :195.0   Median :28.0   Median :53.00   Median :250  
    ##  Mean   :194.9   Mean   :27.2   Mean   :54.41   Mean   :245  
    ##  3rd Qu.:222.0   3rd Qu.:32.0   3rd Qu.:58.00   3rd Qu.:268  
    ##  Max.   :253.0   Max.   :41.0   Max.   :71.00   Max.   :301  
    ##  president_party   
    ##  Length:822        
    ##  Class :character  
    ##  Mode  :character  
    ##                    
    ##                    
    ## 

``` r
summary(snp_clean)
```

    ##       year         month             snp_close      
    ##  Min.   :1950   Length:787         Min.   :  17.05  
    ##  1st Qu.:1966   Class :character   1st Qu.:  83.73  
    ##  Median :1982   Mode  :character   Median : 138.53  
    ##  Mean   :1982                      Mean   : 474.89  
    ##  3rd Qu.:1999                      3rd Qu.: 941.80  
    ##  Max.   :2015                      Max.   :2107.39

``` r
summary(unemployment_clean)
```

    ##       year         month           unemployment_rate
    ##  Min.   :1948   Length:816         Min.   : 2.50    
    ##  1st Qu.:1965   Class :character   1st Qu.: 4.70    
    ##  Median :1982   Mode  :character   Median : 5.60    
    ##  Mean   :1982                      Mean   : 5.83    
    ##  3rd Qu.:1998                      3rd Qu.: 6.90    
    ##  Max.   :2015                      Max.   :10.80    
    ##                                    NA's   :6

After cleaning and tidying, these were merged into a single data frame.
The resulting dataset has 822 rows and 11 columns, spanning the years
1947 to 2015. The key variables in the final tidy dataset are `year`,
`month`, `gov_gop`, `president_party`, `snp_close`, and
`unemployment_rate`, providing a unified view of political and economic
indicators suitable for time-series analysis.

``` r
# View a glimpse of the final merged data
glimpse(fivethirtyeight_df)
```

    ## Rows: 822
    ## Columns: 11
    ## $ year              <dbl> 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947, 1947…
    ## $ month             <chr> "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Au…
    ## $ gov_gop           <dbl> 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 22, …
    ## $ sen_gop           <dbl> 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 53, …
    ## $ rep_gop           <dbl> 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 25…
    ## $ gov_dem           <dbl> 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, …
    ## $ sen_dem           <dbl> 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 48, …
    ## $ rep_dem           <dbl> 198, 198, 198, 198, 198, 198, 198, 198, 198, 198, 19…
    ## $ president_party   <chr> "dem", "dem", "dem", "dem", "dem", "dem", "dem", "de…
    ## $ snp_close         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …
    ## $ unemployment_rate <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3.4,…

# Problem 2

We begin by importing the data for “Mr. Trash Wheel”. The cleaning
process involves several key steps:  
1. We use `read_excel()` and specify the sheet name (Mr. Trash Wheel
sheet). We also skip = 1 to ignore the non-data title row in the file.  
2. `clean_names()` converts messy column headers (like “Weight (tons)”)
into a standard snake_case format (e.g., `weight_tons`).  
3. `drop_na(dumpster)` removes rows that are not actual dumpster
collections (like monthly totals or notes).  
4. We round the `sports_balls` count to the nearest whole number and
convert it to an integer.  
5. Finally, we add a `trash_wheel_name` column to identify the data
source.

``` r
# Define the path to the data file
file_path = "problem2_datasets/202509 Trash Wheel Collection Data.xlsx"

# Clean Mr. Trash Wheel data
mr_trash_wheel_df = 
  read_excel(file_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel_name = "Mr. Trash Wheel"
  )
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
mr_trash_wheel_df
```

    ## # A tibble: 707 × 17
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <int> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 697 more rows
    ## # ℹ 11 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, x15 <lgl>,
    ## #   x16 <lgl>, trash_wheel_name <chr>

Next, we apply the exact same cleaning logic to the data for “Professor
Trash Wheel” and “Gwynnda Trash Wheel”.

``` r
# Clean Professor Trash Wheel data
professor_trash_wheel_df = 
  read_excel(file_path, sheet = "Professor Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Professor Trash Wheel"
  )
professor_trash_wheel_df
```

    ## # A tibble: 132 × 14
    ##    dumpster month     year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>    <int> <dttm>                    <dbl>              <dbl>
    ##  1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ##  2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ##  3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ##  4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ##  5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ##  6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ##  7        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ##  8        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ##  9        9 May       2017 2017-05-10 00:00:00        2.64                 15
    ## 10       10 May       2017 2017-05-26 00:00:00        2.78                 15
    ## # ℹ 122 more rows
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel_name <chr>

``` r
# Clean Gwynnda Trash Wheel data
gwynnda_trash_wheel_df = 
  read_excel(file_path, sheet = "Gwynns Falls Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Gwynnda, the Good Wheel of the West"
  )
gwynnda_trash_wheel_df
```

    ## # A tibble: 349 × 13
    ##    dumpster month   year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr>  <int> <dttm>                    <dbl>              <dbl>
    ##  1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ##  2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ##  3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ##  4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ##  5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ##  6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ##  7        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ##  8        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ##  9        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 10       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## # ℹ 339 more rows
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel_name <chr>

With all three data frames cleaned and uniformly structured, we stack
them into a single, tidy dataset using `bind_rows()`. The `sports_balls`
column will correctly show NA for the trash wheels that do not collect
that data.

``` r
all_trash_wheel_data <- 
  bind_rows(
    mr_trash_wheel_df, 
    professor_trash_wheel_df, 
    gwynnda_trash_wheel_df
  )
all_trash_wheel_data
```

    ## # A tibble: 1,188 × 17
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <int> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,178 more rows
    ## # ℹ 11 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, x15 <lgl>,
    ## #   x16 <lgl>, trash_wheel_name <chr>

``` r
# Use glimpse() to see the structure of the final combined data frame
glimpse(all_trash_wheel_data)
```

    ## Rows: 1,188
    ## Columns: 17
    ## $ dumpster           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …
    ## $ month              <chr> "May", "May", "May", "May", "May", "May", "May", "M…
    ## $ year               <int> 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 201…
    ## $ date               <dttm> 2014-05-16, 2014-05-16, 2014-05-16, 2014-05-17, 20…
    ## $ weight_tons        <dbl> 4.31, 2.74, 3.45, 3.10, 4.06, 2.71, 1.91, 3.70, 2.5…
    ## $ volume_cubic_yards <dbl> 18, 13, 15, 15, 18, 13, 8, 16, 14, 18, 15, 19, 15, …
    ## $ plastic_bottles    <dbl> 1450, 1120, 2450, 2380, 980, 1430, 910, 3580, 2400,…
    ## $ polystyrene        <dbl> 1820, 1030, 3100, 2730, 870, 2140, 1090, 4310, 2790…
    ## $ cigarette_butts    <dbl> 126000, 91000, 105000, 100000, 120000, 90000, 56000…
    ## $ glass_bottles      <dbl> 72, 42, 50, 52, 72, 46, 32, 58, 49, 75, 38, 45, 58,…
    ## $ plastic_bags       <dbl> 584, 496, 1080, 896, 368, 672, 416, 1552, 984, 448,…
    ## $ wrappers           <dbl> 1162, 874, 2032, 1971, 753, 1144, 692, 3015, 1988, …
    ## $ sports_balls       <int> 7, 5, 6, 6, 7, 5, 3, 6, 6, 7, 6, 8, 6, 6, 6, 6, 5, …
    ## $ homes_powered      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
    ## $ x15                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
    ## $ x16                <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…
    ## $ trash_wheel_name   <chr> "Mr. Trash Wheel", "Mr. Trash Wheel", "Mr. Trash Wh…

The final combined dataset provides a comprehensive look at the trash
collected by Baltimore’s trash wheels. It contains a total of 1188
dumpster collection records. Key variables include `trash_wheel_name`,
`weight_tons`, and counts of specific items like `plastic_bottles` and
`cigarette_butts`. Specifically,

``` r
summary(all_trash_wheel_data)
```

    ##     dumpster        month                year     
    ##  Min.   :  1.0   Length:1188        Min.   :2014  
    ##  1st Qu.: 99.0   Class :character   1st Qu.:2018  
    ##  Median :231.0   Mode  :character   Median :2021  
    ##  Mean   :269.2                      Mean   :2020  
    ##  3rd Qu.:410.2                      3rd Qu.:2023  
    ##  Max.   :707.0                      Max.   :2025  
    ##                                                   
    ##       date                      weight_tons    volume_cubic_yards
    ##  Min.   :2014-05-16 00:00:00   Min.   :0.610   Min.   : 5.00     
    ##  1st Qu.:2018-06-12 00:00:00   1st Qu.:2.587   1st Qu.:15.00     
    ##  Median :2021-09-16 12:00:00   Median :3.100   Median :15.00     
    ##  Mean   :2020-12-24 00:41:12   Mean   :3.062   Mean   :15.05     
    ##  3rd Qu.:2023-08-24 00:00:00   3rd Qu.:3.560   3rd Qu.:15.00     
    ##  Max.   :2025-07-29 00:00:00   Max.   :5.620   Max.   :20.00     
    ##                                                                  
    ##  plastic_bottles  polystyrene    cigarette_butts  glass_bottles   
    ##  Min.   :   0    Min.   :    0   Min.   :     0   Min.   :  0.00  
    ##  1st Qu.:1008    1st Qu.:  240   1st Qu.:  3000   1st Qu.: 10.00  
    ##  Median :2000    Median :  530   Median :  4900   Median : 18.00  
    ##  Mean   :2226    Mean   : 1259   Mean   : 12275   Mean   : 21.08  
    ##  3rd Qu.:2900    3rd Qu.: 1645   3rd Qu.:  9000   3rd Qu.: 29.00  
    ##  Max.   :9830    Max.   :11528   Max.   :310000   Max.   :110.00  
    ##  NA's   :1       NA's   :1       NA's   :1        NA's   :350     
    ##   plastic_bags        wrappers      sports_balls   homes_powered  
    ##  Min.   :    0.0   Min.   :    0   Min.   : 0.00   Min.   : 0.00  
    ##  1st Qu.:  240.0   1st Qu.:  960   1st Qu.: 6.00   1st Qu.:40.12  
    ##  Median :  440.0   Median : 1760   Median :13.00   Median :50.25  
    ##  Mean   :  859.7   Mean   : 2653   Mean   :14.76   Mean   :47.21  
    ##  3rd Qu.:  980.0   3rd Qu.: 3422   3rd Qu.:21.00   3rd Qu.:58.33  
    ##  Max.   :13450.0   Max.   :20100   Max.   :56.00   Max.   :93.67  
    ##  NA's   :1         NA's   :118     NA's   :481     NA's   :132    
    ##    x15            x16          trash_wheel_name  
    ##  Mode:logical   Mode:logical   Length:1188       
    ##  NA's:1188      NA's:1188      Class :character  
    ##                                Mode  :character  
    ##                                                  
    ##                                                  
    ##                                                  
    ## 

From this unified dataset, we can answer the specific questions posed in
the Assignment 2:

1.  What was the total weight of trash collected by Professor Trash
    Wheel?

Answer: The total weight collected was 282.26 tons.

2.  What was the total number of cigarette butts collected by Gwynnda in
    June of 2022?

Answer: The total number of cigarette butts collected was 18,120.

# Problem 3

This problem analyzes Zillow’s Observed Rent Index (ZORI) for New York
City from 2015 to 2024. We will import and combine Zillow’s rental price
data with a separate file containing NYC ZIP code and neighborhood
information. The goal is to create a single, tidy dataset to explore
rental price trends, particularly during the COVID-19 pandemic.

The first step is to import, clean, and merge the two datasets. The
Zillow data is in a “wide” format and needs to be reshaped, while the
ZIP code data needs minor cleaning before they can be joined.

``` r
# Import the Zillow and ZIP code datasets
zillow_df <- read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv")
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
zip_codes_df <- read_csv("zillow_data/Zip Codes.csv")
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
# Clean and tidy the Zillow rental data
zori_tidy <- zillow_df |>
  pivot_longer(
    cols = starts_with("20"),
    names_to = "date",
    values_to = "zori"
  ) |>
  mutate(
    date = ymd(date),
    # Ensure ZIP code is a character for a safe merge
    zip_code = as.character(RegionName) 
  ) |>
  select(zip_code, date, zori)
zori_tidy
```

    ## # A tibble: 17,284 × 3
    ##    zip_code date        zori
    ##    <chr>    <date>     <dbl>
    ##  1 11368    2015-01-31    NA
    ##  2 11368    2015-02-28    NA
    ##  3 11368    2015-03-31    NA
    ##  4 11368    2015-04-30    NA
    ##  5 11368    2015-05-31    NA
    ##  6 11368    2015-06-30    NA
    ##  7 11368    2015-07-31    NA
    ##  8 11368    2015-08-31    NA
    ##  9 11368    2015-09-30    NA
    ## 10 11368    2015-10-31    NA
    ## # ℹ 17,274 more rows

``` r
# Clean the ZIP code information dataset 
zip_info_tidy <- zip_codes_df |>
  clean_names() |>
  # Keep only the first occurrence of each ZIP code, remove duplicates
  distinct(zip_code, .keep_all = TRUE) |> 
  mutate(zip_code = as.character(zip_code))
# janitor::get_dupes(zip_info_tidy, zip_code)


# Join the two datasets to create the final, organized dataset
nyc_zori_final <- left_join(zori_tidy, zip_info_tidy, by = "zip_code") |>
  # Reorder columns for clarity
  select(zip_code, county, neighborhood, date, zori) |>
  # Order observations meaningfully
  arrange(zip_code, date)
nyc_zori_final
```

    ## # A tibble: 17,284 × 5
    ##    zip_code county   neighborhood        date        zori
    ##    <chr>    <chr>    <chr>               <date>     <dbl>
    ##  1 10001    New York Chelsea and Clinton 2015-01-31 3855.
    ##  2 10001    New York Chelsea and Clinton 2015-02-28 3892.
    ##  3 10001    New York Chelsea and Clinton 2015-03-31 3898.
    ##  4 10001    New York Chelsea and Clinton 2015-04-30 3970.
    ##  5 10001    New York Chelsea and Clinton 2015-05-31 4033.
    ##  6 10001    New York Chelsea and Clinton 2015-06-30 4071.
    ##  7 10001    New York Chelsea and Clinton 2015-07-31 4067.
    ##  8 10001    New York Chelsea and Clinton 2015-08-31 4070.
    ##  9 10001    New York Chelsea and Clinton 2015-09-30 4040.
    ## 10 10001    New York Chelsea and Clinton 2015-10-31 4023.
    ## # ℹ 17,274 more rows

The resulting tidy dataset combines rental price data with geographic
information for New York City. Each observation represents the Zillow
Observed Rent Index for a single ZIP code on a given month. The dataset
includes a total of 17284 observations. It covers 149 unique ZIP codes
and 43 unique neighborhoods across the five boroughs. Key variables
include `zip_code`, `county`, `neighborhood`, `date`, and the rental
index `zori`. Specifically,

``` r
glimpse(nyc_zori_final)
```

    ## Rows: 17,284
    ## Columns: 5
    ## $ zip_code     <chr> "10001", "10001", "10001", "10001", "10001", "10001", "10…
    ## $ county       <chr> "New York", "New York", "New York", "New York", "New York…
    ## $ neighborhood <chr> "Chelsea and Clinton", "Chelsea and Clinton", "Chelsea an…
    ## $ date         <date> 2015-01-31, 2015-02-28, 2015-03-31, 2015-04-30, 2015-05-…
    ## $ zori         <dbl> 3855.089, 3892.376, 3898.212, 3969.644, 4033.221, 4070.80…

For the second step, not all ZIP codes in New York City have
corresponding rental data in the Zillow dataset. We can identify these
missing ZIP codes using an `anti_join`, it will return all rows from x
without a match in y.

``` r
missing_zips <- anti_join(zip_info_tidy, zori_tidy, by = "zip_code")
missing_zips
```

    ## # A tibble: 171 × 7
    ##    county state_fips county_code county_fips zip_code file_date neighborhood    
    ##    <chr>       <dbl> <chr>             <dbl> <chr>    <chr>     <chr>           
    ##  1 Bronx          36 005               36005 10464    7/25/07   Southeast Bronx 
    ##  2 Bronx          36 005               36005 10474    7/25/07   Hunts Point and…
    ##  3 Bronx          36 005               36005 10475    7/25/07   Northeast Bronx 
    ##  4 Bronx          36 005               36005 10499    7/25/07   <NA>            
    ##  5 Bronx          36 005               36005 10550    7/25/07   <NA>            
    ##  6 Bronx          36 005               36005 10704    7/25/07   <NA>            
    ##  7 Bronx          36 005               36005 10705    7/25/07   <NA>            
    ##  8 Bronx          36 005               36005 10803    7/25/07   <NA>            
    ##  9 Kings          36 047               36047 11202    7/25/07   <NA>            
    ## 10 Kings          36 047               36047 11224    7/25/07   Southern Brookl…
    ## # ℹ 161 more rows

The `anti_join` reveals 171 ZIP codes that appear in the NYC ZIP code
list but not in the Zillow dataset. A few illustrative examples help
explain why this might be the case:

1.  10004 (Financial District, Manhattan): While it contains residential
    buildings like apartments, this ZIP code is dominated by commercial
    office space, ferries (Staten Island Ferry), and tourist attractions
    (the Statue of Liberty, Ellis Island). The low volume of purely
    residential rental units may not be sufficient for Zillow to create
    a stable rental index.

2.  10007 (Financial District, Manhattan): This ZIP code covers the
    World Trade Center complex and City Hall Park. Like 10004, it is a
    mix of commercial, government, and high-end residential buildings,
    which may lead to insufficient or non-representative rental data.

3.  11430 (Jamaica, Queens): This ZIP code is exclusively for John F.
    Kennedy International Airport. As it contains no residential
    addresses, it is correctly excluded from a residential rental index.

In general, ZIP codes are excluded from the Zillow dataset if they are
not primarily residential areas, have too few rental transactions to
create a reliable index, or are special-purpose ZIP codes for landmarks,
airports, or large commercial buildings.

For the last part of problem 3, to analyze the impact of the COVID-19
pandemic, we compare rental prices in January 2021 to those in January
2020. The table below shows the 10 ZIP codes with the largest absolute
drop in the Zillow Observed Rent Index during this period.

``` r
# Filter for Jan 2020 and Jan 2021, and calculate the price drop
price_drop_df <- nyc_zori_final |>
  filter(
    date == ymd("2020-01-31") | date == ymd("2021-01-31")
  ) |>
  pivot_wider(
    names_from = date,
    values_from = zori
  ) |>
  clean_names() |>
  mutate(
    price_drop = x2020_01_31 - x2021_01_31
  ) |>
  # Remove entries where data for one of the dates was missing
  drop_na(price_drop)

# Create a table of the top 10 ZIP codes with the largest price drop
top_10_drops <- price_drop_df |>
  arrange(desc(price_drop)) |>
  head(10) |>
  select(zip_code, county, neighborhood, price_drop)

# Use kable for a nicely formatted table
kable(
  top_10_drops,
  col.names = c("ZIP Code", "County", "Neighborhood", "Price Drop ($)"),
  caption = "Top 10 ZIP Codes by Largest Rent Decrease (Jan 2020 to Jan 2021)",
  digits = 2
)
```

| ZIP Code | County   | Neighborhood                  | Price Drop (\$) |
|:---------|:---------|:------------------------------|----------------:|
| 10007    | New York | Lower Manhattan               |          912.60 |
| 10069    | New York | NA                            |          748.12 |
| 10009    | New York | Lower East Side               |          714.25 |
| 10016    | New York | Gramercy Park and Murray Hill |          711.70 |
| 10001    | New York | Chelsea and Clinton           |          710.45 |
| 10002    | New York | Lower East Side               |          710.30 |
| 10004    | New York | Lower Manhattan               |          705.96 |
| 10038    | New York | Lower Manhattan               |          697.59 |
| 10012    | New York | Greenwich Village and Soho    |          686.22 |
| 10010    | New York | Gramercy Park and Murray Hill |          684.93 |

Top 10 ZIP Codes by Largest Rent Decrease (Jan 2020 to Jan 2021)

The table clearly shows that the 10 ZIP codes with the most significant
drop in rental prices are all located in Manhattan, specifically in and
around the Financial District, Midtown, and other high-end neighborhoods
like Chelsea and the West Village. This aligns with the widely reported
trend of residents moving out of Manhattan’s expensive core during the
height of the COVID-19 pandemic, driven by the shift to remote work and
a desire for more space. The areas that were previously most in-demand
due to their proximity to offices and entertainment saw the largest
immediate impact on rental demand, leading to a substantial, albeit
temporary, drop in prices.
