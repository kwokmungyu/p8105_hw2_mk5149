---
title: "p8105_hw2_mk5149"
author: "Mungyu Kwok"
date: "2025-09-30"
output: github_document
---
# Preparation
Firstly, we will load any necessary packages.
```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(janitor)

library(knitr)
```

Next, we will load the datasets (Fivethirtyeight), and save them into dataframe.
```{r}
pols_df = read_csv("fivethirtyeight_datasets/pols-month.csv")
snp_df = read_csv("fivethirtyeight_datasets/snp.csv")
unemployment_df = read_csv("fivethirtyeight_datasets/unemployment.csv")
```

# Problem 1
First, clean the data in `pols-month.csv`. 
We will use the `separate()` function to break up the variable `mon` into integer variables `year`, `month`, and `day`.
Then, we will replace month number with month name; 
We will also create a president variable taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; 
Finally, we will remove the `day` variable.
```{r}
pols_clean = 
  pols_df |>
  separate(mon, into = c("year","month","day"), sep = '-', convert = TRUE) |> 
  mutate(
    month = month.abb[month], #buildt-in R constant
    president_party = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) |>
  select(-day, -prez_gop, -prez_dem) # Removes only the day, prez_gop, and prez_dem columns
pols_clean
```

Secondly, we will clean the data in `snp.csv` by using a similar process to the above. 
For consistency across datasets, arrange according to year and month, and organize so that `year` and `month` are the leading columns.
```{r}
snp_clean = 
  snp_df |>
  mutate(
    date = mdy(date),
    year = year(date),
    month = month(date, label = TRUE, abbr = TRUE) |>
      as.character()
  ) |>
  select(year, month, snp_close = close)
snp_clean
```

Thirdly, we will tidy the unemployment data so that it can be merged with the previous datasets. 
This process will involve switching from “wide” to “long” format; 
ensuring that key variables have the same name; 
and ensuring that key variables take the same values.
```{r}
unemployment_clean = 
  unemployment_df |>
  pivot_longer(
    cols = Jan:Dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |> 
  rename(year = Year) # Add this line to fix the capitalization, which will cause error in the later part (merging the data)
unemployment_clean
```

For the final part of problem 1, we will join the datasets by merging `snp` into `pols`, and merging `unemployment` into the result.
```{r}
fivethirtyeight_df = 
  pols_clean |>
  left_join(snp_clean, by = c("year","month")) |>
  left_join(unemployment_clean, by = c("year", "month"))
#left_join: based on shared columns, if no matches -> NA
```

The three raw datasets from FiveThirtyEight provided distinct economic and political metrics over time. The `pols-month.csv` dataset contained monthly presidential approval ratings from 1947 to 2017, with indicators for the president's political party. The `snp.csv` dataset included the monthly closing value of the S&P 500 index. Lastly, the `unemployment.csv` dataset was in a wide format, listing the national unemployment rate for each month in separate columns, covering years from 1948 to 2015. 

For the specific information for these 3 datasets, please refer to the summary below: 
```{r}
summary(pols_clean)
```

```{r}
summary(snp_clean)
```

```{r}
summary(unemployment_clean)
```


After cleaning and tidying, these were merged into a single data frame. The resulting dataset has 822 rows and 11 columns, spanning the years 1947 to 2015. The key variables in the final tidy dataset are `year`, `month`, `gov_gop`, `president_party`, `snp_close`, and `unemployment_rate`, providing a unified view of political and economic indicators suitable for time-series analysis.
```{r}
# View a glimpse of the final merged data
glimpse(fivethirtyeight_df)
```

# Problem 2
We begin by importing the data for "Mr. Trash Wheel". The cleaning process involves several key steps:  
1. We use `read_excel()` and specify the sheet name (Mr. Trash Wheel sheet). We also skip = 1 to ignore the non-data title row in the file.  
2. `clean_names()` converts messy column headers (like "Weight (tons)") into a standard snake_case format (e.g., `weight_tons`).  
3. `drop_na(dumpster)` removes rows that are not actual dumpster collections (like monthly totals or notes).  
4. We round the `sports_balls` count to the nearest whole number and convert it to an integer.  
5. Finally, we add a `trash_wheel_name` column to identify the data source.  
```{r}
# Define the path to the data file
file_path = "problem2_datasets/202409 Trash Wheel Collection Data.xlsx"

# Clean Mr. Trash Wheel data
mr_trash_wheel_df = 
  read_excel(file_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel_name = "Mr. Trash Wheel"
  )
mr_trash_wheel_df
```

Next, we apply the exact same cleaning logic to the data for "Professor Trash Wheel" and "Gwynnda Trash Wheel".
```{r}
# Clean Professor Trash Wheel data
professor_trash_wheel_df = 
  read_excel(file_path, sheet = "Professor Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Professor Trash Wheel"
  )
professor_trash_wheel_df

# Clean Gwynnda Trash Wheel data
gwynnda_trash_wheel_df = 
  read_excel(file_path, sheet = "Gwynnda Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Gwynnda, the Good Wheel of the West"
  )
gwynnda_trash_wheel_df
```
  
With all three data frames cleaned and uniformly structured, we stack them into a single, tidy dataset using `bind_rows()`. The `sports_balls` column will correctly show NA for the trash wheels that do not collect that data.
```{r}
all_trash_wheel_data <- 
  bind_rows(
    mr_trash_wheel_df, 
    professor_trash_wheel_df, 
    gwynnda_trash_wheel_df
  )
all_trash_wheel_data

# Use glimpse() to see the structure of the final combined data frame
glimpse(all_trash_wheel_data)
```

The final combined dataset provides a comprehensive look at the trash collected by Baltimore's trash wheels. It contains a total of `r nrow(all_trash_wheel_data)` dumpster collection records. Key variables include `trash_wheel_name`, `weight_tons`, and counts of specific items like `plastic_bottles` and `cigarette_butts`. Specifically,
```{r}
summary(all_trash_wheel_data)
```


From this unified dataset, we can answer the specific questions posed in the Assignment 2:  

1. What was the total weight of trash collected by Professor Trash Wheel?  

Answer: The total weight collected was `r all_trash_wheel_data |> filter(trash_wheel_name == "Professor Trash Wheel") |> summarize(total_weight = sum(weight_tons, na.rm = TRUE)) |> pull(total_weight) |> round(2)` tons.  

2. What was the total number of cigarette butts collected by Gwynnda in June of 2022?  

Answer: The total number of cigarette butts collected was `r all_trash_wheel_data |> filter(trash_wheel_name == "Gwynnda, the Good Wheel of the West", year == 2022, month == "June") |> pull(cigarette_butts)`.  

# Problem 3
This problem analyzes Zillow's Observed Rent Index (ZORI) for New York City from 2015 to 2024. We will import and combine Zillow's rental price data with a separate file containing NYC ZIP code and neighborhood information. The goal is to create a single, tidy dataset to explore rental price trends, particularly during the COVID-19 pandemic.














