---
title: "p8105_hw2_mk5149"
author: "Mungyu Kwok"
date: "2025-09-30"
output: github_document
---
# Preparation
Firstly, we will load any necessary packages.
```{r}
library(tidyverse)
library(lubridate)
library(readxl)
library(janitor)

library(knitr)
```

Next, we will load the datasets (Fivethirtyeight), and save them into dataframe.
```{r}
pols_df = read_csv("fivethirtyeight_datasets/pols-month.csv")
snp_df = read_csv("fivethirtyeight_datasets/snp.csv")
unemployment_df = read_csv("fivethirtyeight_datasets/unemployment.csv")
```

# Problem 1
First, clean the data in `pols-month.csv`. 
We will use the `separate()` function to break up the variable `mon` into integer variables `year`, `month`, and `day`.
Then, we will replace month number with month name; 
We will also create a president variable taking values `gop` and `dem`, and remove `prez_dem` and `prez_gop`; 
Finally, we will remove the `day` variable.
```{r}
pols_clean = 
  pols_df |>
  separate(mon, into = c("year","month","day"), sep = '-', convert = TRUE) |> 
  mutate(
    month = month.abb[month], #buildt-in R constant
    president_party = case_when(
      prez_gop == 1 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) |>
  select(-day, -prez_gop, -prez_dem) # Removes only the day, prez_gop, and prez_dem columns
pols_clean
```

Secondly, we will clean the data in `snp.csv` by using a similar process to the above. 
For consistency across datasets, arrange according to year and month, and organize so that `year` and `month` are the leading columns.
```{r}
snp_clean = 
  snp_df |>
  mutate(
    date = mdy(date),
    year = year(date),
    month = month(date, label = TRUE, abbr = TRUE) |>
      as.character()
  ) |>
  select(year, month, snp_close = close)
snp_clean
```

Thirdly, we will tidy the unemployment data so that it can be merged with the previous datasets. 
This process will involve switching from “wide” to “long” format; 
ensuring that key variables have the same name; 
and ensuring that key variables take the same values.
```{r}
unemployment_clean = 
  unemployment_df |>
  pivot_longer(
    cols = Jan:Dec,
    names_to = "month",
    values_to = "unemployment_rate"
  ) |> 
  rename(year = Year) # Add this line to fix the capitalization, which will cause error in the later part (merging the data)
unemployment_clean
```

For the final part of problem 1, we will join the datasets by merging `snp` into `pols`, and merging `unemployment` into the result.
```{r}
fivethirtyeight_df = 
  pols_clean |>
  left_join(snp_clean, by = c("year","month")) |>
  left_join(unemployment_clean, by = c("year", "month"))
#left_join: based on shared columns, if no matches -> NA
```

The three raw datasets from FiveThirtyEight provided distinct economic and political metrics over time. The `pols-month.csv` dataset contained monthly presidential approval ratings from 1947 to 2017, with indicators for the president's political party. The `snp.csv` dataset included the monthly closing value of the S&P 500 index. Lastly, the `unemployment.csv` dataset was in a wide format, listing the national unemployment rate for each month in separate columns, covering years from 1948 to 2015. 

For the specific information for these 3 datasets, please refer to the summary below: 
```{r}
summary(pols_clean)
```

```{r}
summary(snp_clean)
```

```{r}
summary(unemployment_clean)
```


After cleaning and tidying, these were merged into a single data frame. The resulting dataset has 822 rows and 11 columns, spanning the years 1947 to 2015. The key variables in the final tidy dataset are `year`, `month`, `gov_gop`, `president_party`, `snp_close`, and `unemployment_rate`, providing a unified view of political and economic indicators suitable for time-series analysis.
```{r}
# View a glimpse of the final merged data
glimpse(fivethirtyeight_df)
```

# Problem 2
We begin by importing the data for "Mr. Trash Wheel". The cleaning process involves several key steps:  
1. We use `read_excel()` and specify the sheet name (Mr. Trash Wheel sheet). We also skip = 1 to ignore the non-data title row in the file.  
2. `clean_names()` converts messy column headers (like "Weight (tons)") into a standard snake_case format (e.g., `weight_tons`).  
3. `drop_na(dumpster)` removes rows that are not actual dumpster collections (like monthly totals or notes).  
4. We round the `sports_balls` count to the nearest whole number and convert it to an integer.  
5. Finally, we add a `trash_wheel_name` column to identify the data source.  
```{r}
# Define the path to the data file
file_path = "problem2_datasets/202409 Trash Wheel Collection Data.xlsx"

# Clean Mr. Trash Wheel data
mr_trash_wheel_df = 
  read_excel(file_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel_name = "Mr. Trash Wheel"
  )
mr_trash_wheel_df
```

Next, we apply the exact same cleaning logic to the data for "Professor Trash Wheel" and "Gwynnda Trash Wheel".
```{r}
# Clean Professor Trash Wheel data
professor_trash_wheel_df = 
  read_excel(file_path, sheet = "Professor Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Professor Trash Wheel"
  )
professor_trash_wheel_df

# Clean Gwynnda Trash Wheel data
gwynnda_trash_wheel_df = 
  read_excel(file_path, sheet = "Gwynnda Trash Wheel", skip = 1) |>
  clean_names() |>
  drop_na(dumpster) |>
  mutate(
    year = as.integer(year),
    trash_wheel_name = "Gwynnda, the Good Wheel of the West"
  )
gwynnda_trash_wheel_df
```
  
With all three data frames cleaned and uniformly structured, we stack them into a single, tidy dataset using `bind_rows()`. The `sports_balls` column will correctly show NA for the trash wheels that do not collect that data.
```{r}
all_trash_wheel_data <- 
  bind_rows(
    mr_trash_wheel_df, 
    professor_trash_wheel_df, 
    gwynnda_trash_wheel_df
  )
all_trash_wheel_data

# Use glimpse() to see the structure of the final combined data frame
glimpse(all_trash_wheel_data)
```

The final combined dataset provides a comprehensive look at the trash collected by Baltimore's trash wheels. It contains a total of `r nrow(all_trash_wheel_data)` dumpster collection records. Key variables include `trash_wheel_name`, `weight_tons`, and counts of specific items like `plastic_bottles` and `cigarette_butts`. Specifically,
```{r}
summary(all_trash_wheel_data)
```


From this unified dataset, we can answer the specific questions posed in the Assignment 2:  

1. What was the total weight of trash collected by Professor Trash Wheel?  

Answer: The total weight collected was `r all_trash_wheel_data |> filter(trash_wheel_name == "Professor Trash Wheel") |> summarize(total_weight = sum(weight_tons, na.rm = TRUE)) |> pull(total_weight) |> round(2)` tons.  

2. What was the total number of cigarette butts collected by Gwynnda in June of 2022?  

Answer: The total number of cigarette butts collected was `r all_trash_wheel_data |> filter(trash_wheel_name == "Gwynnda, the Good Wheel of the West", year == 2022, month == "June") |> pull(cigarette_butts)`.  

# Problem 3
This problem analyzes Zillow's Observed Rent Index (ZORI) for New York City from 2015 to 2024. We will import and combine Zillow's rental price data with a separate file containing NYC ZIP code and neighborhood information. The goal is to create a single, tidy dataset to explore rental price trends, particularly during the COVID-19 pandemic.

The first step is to import, clean, and merge the two datasets. The Zillow data is in a "wide" format and needs to be reshaped, while the ZIP code data needs minor cleaning before they can be joined.
```{r}
# Import the Zillow and ZIP code datasets
zillow_df <- read_csv("zillow_data/Zip_zori_uc_sfrcondomfr_sm_month_NYC.csv")
zip_codes_df <- read_csv("zillow_data/Zip Codes.csv")

# Clean and tidy the Zillow rental data
zori_tidy <- zillow_df |>
  pivot_longer(
    cols = starts_with("20"),
    names_to = "date",
    values_to = "zori"
  ) |>
  mutate(
    date = ymd(date),
    # Ensure ZIP code is a character for a safe merge
    zip_code = as.character(RegionName) 
  ) |>
  select(zip_code, date, zori)
zori_tidy

# Clean the ZIP code information dataset
# Clean the ZIP code information dataset (Corrected)
zip_info_tidy <- zip_codes_df |>
  clean_names() |>
  mutate(zip_code = as.character(zip_code))
zip_info_tidy

# Join the two datasets to create the final, organized dataset
nyc_zori_final <- left_join(zori_tidy, zip_info_tidy, by = "zip_code") |>
  # Reorder columns for clarity
  select(zip_code, county, neighborhood, date, zori) |>
  # Order observations meaningfully
  arrange(zip_code, date)
nyc_zori_final
```
The resulting tidy dataset combines rental price data with geographic information for New York City. Each observation represents the Zillow Observed Rent Index for a single ZIP code on a given month. The dataset includes a total of `r nrow(nyc_zori_final)` observations. It covers `r n_distinct(nyc_zori_final$zip_code)` unique ZIP codes and `r n_distinct(nyc_zori_final$neighborhood)` unique neighborhoods across the five boroughs. Key variables include `zip_code`, `county`, `neighborhood`, `date`, and the rental index `zori`. 
Specifically,
```{r}
glimpse(nyc_zori_final)
```

For the second step, not all ZIP codes in New York City have corresponding rental data in the Zillow dataset. We can identify these missing ZIP codes using an `anti_join`, it will return all rows from x without a match in y.
```{r}
missing_zips <- anti_join(zip_info_tidy, zori_tidy, by = "zip_code")
missing_zips
```

The `anti_join` reveals `r nrow(missing_zips)` ZIP codes that appear in the NYC ZIP code list but not in the Zillow dataset. A few illustrative examples help explain why this might be the case:  

1. 10004 (Financial District, Manhattan): While it contains residential buildings like apartments, this ZIP code is dominated by commercial office space, ferries (Staten Island Ferry), and tourist attractions (the Statue of Liberty, Ellis Island). The low volume of purely residential rental units may not be sufficient for Zillow to create a stable rental index.  

2. 10007 (Financial District, Manhattan): This ZIP code covers the World Trade Center complex and City Hall Park. Like 10004, it is a mix of commercial, government, and high-end residential buildings, which may lead to insufficient or non-representative rental data.  

3. 11430 (Jamaica, Queens): This ZIP code is exclusively for John F. Kennedy International Airport. As it contains no residential addresses, it is correctly excluded from a residential rental index.  

In general, ZIP codes are excluded from the Zillow dataset if they are not primarily residential areas, have too few rental transactions to create a reliable index, or are special-purpose ZIP codes for landmarks, airports, or large commercial buildings.


For the last part of problem 3, to analyze the impact of the COVID-19 pandemic, we compare rental prices in January 2021 to those in January 2020. The table below shows the 10 ZIP codes with the largest absolute drop in the Zillow Observed Rent Index during this period.
```{r}
# Filter for Jan 2020 and Jan 2021, and calculate the price drop
price_drop_df <- nyc_zori_final |>
  filter(
    date == ymd("2020-01-31") | date == ymd("2021-01-31")
  ) |>
  pivot_wider(
    names_from = date,
    values_from = zori
  ) |>
  clean_names() |>
  mutate(
    price_drop = x2020_01_31 - x2021_01_31
  ) |>
  # Remove entries where data for one of the dates was missing
  drop_na(price_drop)

# Create a table of the top 10 ZIP codes with the largest price drop
top_10_drops <- price_drop_df |>
  arrange(desc(price_drop)) |>
  head(10) |>
  select(zip_code, county, neighborhood, price_drop)

# Use kable for a nicely formatted table
kable(
  top_10_drops,
  col.names = c("ZIP Code", "County", "Neighborhood", "Price Drop ($)"),
  caption = "Top 10 ZIP Codes by Largest Rent Decrease (Jan 2020 to Jan 2021)",
  digits = 2
)
```
The table clearly shows that the 10 ZIP codes with the most significant drop in rental prices are all located in Manhattan, specifically in and around the Financial District, Midtown, and other high-end neighborhoods like Chelsea and the West Village. This aligns with the widely reported trend of residents moving out of Manhattan's expensive core during the height of the COVID-19 pandemic, driven by the shift to remote work and a desire for more space. The areas that were previously most in-demand due to their proximity to offices and entertainment saw the largest immediate impact on rental demand, leading to a substantial, albeit temporary, drop in prices.









